{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from data.interaction import Interaction\n",
    "from torch.nn.utils import rnn as rnn_utils\n",
    "import os\n",
    "import pandas as pd\n",
    "from root import DATASET_DIR, absolute, ROOT_DIR\n",
    "from enum import Enum\n",
    "from typing import Union, Dict, List\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "class FeatType(Enum):\n",
    "    Token = (0, \"单个离散特征序列\")\n",
    "    TokenSeq = (1, \"多个离散特征序列\")\n",
    "    Float = (2, \"单个连续特征序列\")\n",
    "    FloatSeq = (3, \"多个连续特征序列\")\n",
    "    \n",
    "    @classmethod\n",
    "    def from_code(cls, code:Union[str, int]):\n",
    "        if isinstance(code, str): code = int(code)\n",
    "        for feat_type in FeatType:\n",
    "            if feat_type.value[0] == code:\n",
    "                return feat_type\n",
    "        return None\n",
    "\n",
    "class RecboleDataset(TorchDataset):\n",
    "    def __init__(self, config) -> None:\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self._get_preset()\n",
    "        self._get_field_from_config()\n",
    "        self._load_data(DATASET_DIR)\n",
    "        self._data_processing()\n",
    "        \n",
    "        \n",
    "    def _get_field_from_config(self):\n",
    "        \"\"\"初始化数据集的通用字段\"\"\"\n",
    "        self.dataset_name = self.config['dataset']\n",
    "        self.uid_field = self.config[\"USER_ID_FIELD\"]\n",
    "        self.iid_field = self.config[\"ITEM_ID_FIELD\"]\n",
    "        self.label_field = self.config[\"LABEL_FIELD\"]\n",
    "        \n",
    "        self.split_ratio = self.config['split_ratio']\n",
    "    \n",
    "    def _get_preset(self):\n",
    "        self.field2type:Dict[str, FeatType] = {}\n",
    "        self.field2num:Dict[str, int] = {}\n",
    "        \n",
    "    def _data_processing(self):\n",
    "        self.feat_name_list = self._build_feat_name_list()\n",
    "        \n",
    "    def _load_data(self, data_dir):\n",
    "        \"\"\"加载数据集\"\"\"\n",
    "        dataset_dir = os.path.join(data_dir, self.dataset_name)\n",
    "        self._load_item_feat(dataset_dir, self.dataset_name)\n",
    "        self._load_user_feat(dataset_dir, self.dataset_name)\n",
    "        self._load_inter_feat(dataset_dir, self.dataset_name)\n",
    "        \n",
    "        \n",
    "    def _load_feat(self, feat_dir, feat_name):\n",
    "        path = os.path.join(feat_dir, feat_name)\n",
    "        if not os.path.exists(path): raise FileNotFoundError(f\"{path} not found\")\n",
    "        df = pd.read_csv(\n",
    "            path, \n",
    "            header=0\n",
    "        )\n",
    "        new_columns = []\n",
    "        for col in df.columns:\n",
    "            name, dtype = col.split(\":\")\n",
    "            dtype = FeatType.from_code(dtype)\n",
    "            if dtype is None: raise ValueError(f\"feat type {dtype} not found\")\n",
    "            new_columns.append(name)    \n",
    "            self.field2type[name] = dtype\n",
    "        df.columns = new_columns\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @property\n",
    "    def uid_num(self):\n",
    "        return self._count_unique(self.uid_field)\n",
    "    \n",
    "    @property\n",
    "    def iid_num(self):\n",
    "        return self._count_unique(self.iid_field)\n",
    "    \n",
    "    def _count_unique(self, feat_name):\n",
    "        return len(self.inter_feat[feat_name].unique())\n",
    "\n",
    "        \n",
    "    def _load_inter_feat(self, feat_dir, feat_prefix):\n",
    "        feat_name = f\"{feat_prefix}.inter\"\n",
    "        self.inter_feat = self._load_feat(feat_dir, feat_name)\n",
    "        \n",
    "    \n",
    "    def _load_user_feat(self, feat_dir, feat_prefix):\n",
    "        feat_name = f\"{feat_prefix}.user\"\n",
    "        self.user_feat = self._load_feat(feat_dir, feat_name)\n",
    "    \n",
    "    def _load_item_feat(self, feat_dir, feat_prefix):\n",
    "        feat_name = f\"{feat_prefix}.item\"\n",
    "        self.item_feat = self._load_feat(feat_dir, feat_name)\n",
    "        \n",
    "    def _build_feat_name_list(self):\n",
    "        feat_name_list = [\n",
    "            feat_name\n",
    "            for feat_name in [\"inter_feat\", \"user_feat\", \"item_feat\"]\n",
    "            if getattr(self, feat_name, None) is not None\n",
    "        ]\n",
    "        return feat_name_list\n",
    "    \n",
    "    def _change_feat_format(self):\n",
    "        for feat_name in self.feat_name_list:\n",
    "            feat = getattr(self, feat_name)\n",
    "            setattr(self, feat_name, self._dataframe_to_interaction(feat))\n",
    "    \n",
    "    def build(self):\n",
    "        self._change_feat_format()\n",
    "        dataset = self.split_by_ratio_without_eval(self.split_ratio)\n",
    "        return dataset\n",
    "        \n",
    "    def split_by_ratio_without_eval(self, split_ratio:float):\n",
    "        \"\"\"分割训练集和测试集\"\"\"\n",
    "        assert 0 < split_ratio < 1\n",
    "        total_cnt = self.__len__()\n",
    "        split_ids = self._calcu_split_ids(total_cnt, [split_ratio, 1 - split_ratio])\n",
    "        next_index = [\n",
    "            range(start, end)\n",
    "            for start, end in zip([0] + split_ids, split_ids + [total_cnt])\n",
    "        ]\n",
    "        next_df = [self.inter_feat[index] for index in next_index]\n",
    "        next_ds = [self.copy(_) for _ in next_df]\n",
    "        return next_ds\n",
    "        \n",
    "        \n",
    "    def copy(self, new_inter_feat):\n",
    "        \"\"\"Given a new interaction feature, return a new :class:`Dataset` object,\n",
    "        whose interaction feature is updated with ``new_inter_feat``, and all the other attributes the same.\n",
    "\n",
    "        Args:\n",
    "            new_inter_feat (Interaction): The new interaction feature need to be updated.\n",
    "\n",
    "        Returns:\n",
    "            :class:`~Dataset`: the new :class:`~Dataset` object, whose interaction feature has been updated.\n",
    "        \"\"\"\n",
    "        nxt = copy.copy(self)\n",
    "        nxt.inter_feat = new_inter_feat\n",
    "        return nxt\n",
    "        \n",
    "        \n",
    "\n",
    "    def _calcu_split_ids(self, tot, ratios):\n",
    "        \"\"\"Given split ratios, and total number, calculate the number of each part after splitting.\n",
    "\n",
    "        Other than the first one, each part is rounded down.\n",
    "\n",
    "        Args:\n",
    "            tot (int): Total number.\n",
    "            ratios (list): List of split ratios. No need to be normalized.\n",
    "\n",
    "        Returns:\n",
    "            list: Number of each part after splitting.\n",
    "        \"\"\"\n",
    "        cnt = [int(ratios[i] * tot) for i in range(len(ratios))]\n",
    "        cnt[0] = tot - sum(cnt[1:])\n",
    "        for i in range(1, len(ratios)):\n",
    "            if cnt[0] <= 1:\n",
    "                break\n",
    "            if 0 < ratios[-i] * tot < 1:\n",
    "                cnt[-i] += 1\n",
    "                cnt[0] -= 1\n",
    "        split_ids = np.cumsum(cnt)[:-1]\n",
    "        return list(split_ids)\n",
    "        \n",
    "    def _dataframe_to_interaction(self, data:pd.DataFrame):\n",
    "        data_for_tensor = {}\n",
    "        for col_name in data:\n",
    "            assert isinstance(col_name, str)\n",
    "            value = data[col_name].values\n",
    "            ftype = self.field2type[col_name]\n",
    "            if ftype == FeatType.Token:\n",
    "                data_for_tensor[col_name] = torch.LongTensor(value)\n",
    "            elif ftype == FeatType.Float:\n",
    "                data_for_tensor[col_name] = torch.FloatTensor(value)\n",
    "            else:\n",
    "                raise NotImplementedError(f\"feat type {ftype} not implemented\")\n",
    "        return Interaction(data_for_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inter_feat)\n",
    "    \n",
    "    def __getitem__(self, index, join=True):\n",
    "        df = self.inter_feat[index]\n",
    "        return self.join(df) if join else df\n",
    "\n",
    "    def join(self, df):\n",
    "        \"\"\"Given interaction feature, join user/item feature into it.\n",
    "\n",
    "        Args:\n",
    "            df (Interaction): Interaction feature to be joint.\n",
    "\n",
    "        Returns:\n",
    "            Interaction: Interaction feature after joining operation.\n",
    "        \"\"\"\n",
    "        if self.user_feat is not None and self.uid_field in df:\n",
    "            df.update(self.user_feat[df[self.uid_field]])\n",
    "        if self.item_feat is not None and self.iid_field in df:\n",
    "            df.update(self.item_feat[df[self.iid_field]])\n",
    "        return df\n",
    "\n",
    "\n",
    "    \n",
    "class GeneralDataset(RecboleDataset):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "\n",
    "\n",
    "config_test = {\n",
    "    \"split_rate\": 0.2,\n",
    "    \"dataset\": \"wsdream-rt\",\n",
    "    \"USER_ID_FIELD\": \"user_id\",\n",
    "    \"ITEM_ID_FIELD\": \"item_id\",\n",
    "    \"LABEL_FIELD\": \"rt\"\n",
    "}\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把原始的wsdream数据转成原子形式\n",
    "# https://recbole.io/cn/atomic_files.html\n",
    "\n",
    "import pandas as pd\n",
    "from root import ORIGINAL_DATASET_DIR, absolute, DATASET_DIR\n",
    "import os\n",
    "from enum import Enum\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class WSDreamDataType(Enum):\n",
    "    TP_ONLY = (1, \"wsdream-tp\")\n",
    "    RT_ONLY = (2, \"wsdream-rt\")\n",
    "    TP_AND_RT = (3, \"wsdream-all\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_code(cls, code:int):\n",
    "        for wsdream_type in WSDreamDataType:\n",
    "            if wsdream_type.value[0] == code:\n",
    "                return wsdream_type\n",
    "        return None\n",
    "\n",
    "class BasicDataConvert:\n",
    "\n",
    "    def load_user_data(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def loda_item_data(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def load_inter_data(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def fit(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "ALL_USER_FIELD = [\"[User ID]\", \"[IP Address]\", \"[Country]\", \"[IP No.]\", \"[AS]\", \"[Latitude]\", \"[Longitude]\"]\n",
    "ALL_ITEM_FIELD = [\"[Service ID]\",\"[WSDL Address]\",\"[Service Provider]\",\"[IP Address]\",\"[Country]\",\"[IP No.]\",\"[AS]\",\"[Latitude]\",\"[Longitude]\"]\n",
    "    \n",
    "class WSDreamDataConvert(BasicDataConvert):\n",
    "    \n",
    "    def __init__(self, wsdream_type:WSDreamDataType) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.origin_user_field = [\"[User ID]\", \"[Country]\", \"[AS]\"]\n",
    "        self.user_field = [\"user_id\", \"country\", \"AS\"]\n",
    "        \n",
    "        self.origin_item_field = [\"[Service ID]\", \"[Country]\", \"[AS]\"]\n",
    "        self.item_field = [\"item_id\", \"country\", \"AS\",]\n",
    "        \n",
    "        self.inter_field = [\"user_id\", \"item_id\"]\n",
    "        if wsdream_type == WSDreamDataType.RT_ONLY: self.inter_field.append(\"rt\")\n",
    "        elif wsdream_type == WSDreamDataType.TP_ONLY: self.inter_field.append(\"tp\")\n",
    "        else: self.inter_field.extend([\"rt\", \"tp\"])\n",
    "            \n",
    "        self.upath = os.path.join(ORIGINAL_DATASET_DIR, \"userlist.txt\")\n",
    "        self.ipath = os.path.join(ORIGINAL_DATASET_DIR, \"wslist.txt\")\n",
    "        self.rt_inter = os.path.join(ORIGINAL_DATASET_DIR, \"rtMatrix.txt\")\n",
    "        self.tp_inter = os.path.join(ORIGINAL_DATASET_DIR, \"tpMatrix.txt\")\n",
    "        \n",
    "        self.wstype = wsdream_type\n",
    "        self.output_dir = os.path.join(DATASET_DIR, wsdream_type.value[1])\n",
    "        \n",
    "        self.dataset_name = wsdream_type.value[1]\n",
    "        \n",
    "        self._load_data()\n",
    "        \n",
    "    def _load_data(self):\n",
    "        self.user_data = self.load_user_data()\n",
    "        self.item_data = self.loda_item_data()\n",
    "        self.inter_data = self.load_inter_data()\n",
    "        for name in [\"[Country]\", \"[AS]\"]:\n",
    "            self._deal_categorical_feat(name)\n",
    "        \n",
    "    def _deal_categorical_feat(self, name:str):\n",
    "        if self.item_data is None and self.user_data is None: self._load_data()\n",
    "        feat_kinds = []\n",
    "        if name in self.user_data: feat_kinds.extend(self.user_data[name].unique().tolist())\n",
    "        if name in self.item_data: feat_kinds.extend(self.item_data[name].unique().tolist())\n",
    "        feat_kinds = list(set(feat_kinds))\n",
    "        map_ = {\n",
    "            feat:idx for idx, feat in enumerate(feat_kinds)\n",
    "        }\n",
    "        if name in self.user_data: self.user_data.replace({name:map_}, inplace=True)\n",
    "        if name in self.item_data: self.item_data.replace({name:map_}, inplace=True)\n",
    "\n",
    "        \n",
    "    def _feat_type_wrap(self, type_:str):\n",
    "        feat_types = []\n",
    "        if type_ == \"user\":\n",
    "            feat_types = [0, 0, 0]\n",
    "            return list(map(lambda x,y:f'{x}:{y}', self.user_field, feat_types))\n",
    "        elif type_ == \"item\":\n",
    "            feat_types = [0, 0, 0]\n",
    "            return list(map(lambda x,y:f'{x}:{y}', self.item_field, feat_types))\n",
    "        else:\n",
    "            if self.wstype == WSDreamDataType.RT_ONLY or self.wstype == WSDreamDataType.TP_ONLY:\n",
    "                feat_types = [0, 0, 2]\n",
    "            else:\n",
    "                feat_types = [0, 0, 2, 2]\n",
    "            return list(map(lambda x,y:f'{x}:{y}', self.inter_field, feat_types))\n",
    "    \n",
    "\n",
    "    def load_inter_data(self) -> pd.DataFrame:\n",
    "        rt_path, tp_path = None, None\n",
    "        if self.wstype == WSDreamDataType.RT_ONLY: rt_path = self.rt_inter\n",
    "        elif self.wstype == WSDreamDataType.TP_ONLY: tp_path = self.tp_inter\n",
    "        else: rt_path, tp_path = self.rt_inter, self.tp_inter\n",
    "        if rt_path and tp_path:\n",
    "            rt_data = np.loadtxt(rt_path, dtype=np.float64)\n",
    "            tp_data = np.loadtxt(tp_path, dtype=np.float64)\n",
    "            rows, cols = np.nonzero(rt_data)\n",
    "            inter_data = pd.DataFrame({self.inter_field[0]:rows, self.inter_field[1]:cols, self.inter_field[2]:rt_data[rows, cols], self.inter_field[3]: tp_data[rows, cols]})\n",
    "        else:\n",
    "            path = self.rt_inter if rt_path else self.tp_inter\n",
    "            inter_data = np.loadtxt(path, dtype=np.float64)\n",
    "            rows, cols = np.nonzero(inter_data)\n",
    "            inter_data = pd.DataFrame({self.inter_field[0]:rows, self.inter_field[1]:cols, self.inter_field[2]:inter_data[rows, cols]})\n",
    "        return inter_data\n",
    "\n",
    "    def load_user_data(self) -> pd.DataFrame:\n",
    "        return pd.read_csv(self.upath, sep=\"\\t\", header=0)[self.origin_user_field]\n",
    "    \n",
    "    def loda_item_data(self):\n",
    "        return pd.read_csv(self.ipath, sep=\"\\t\", header=0)[self.origin_item_field]\n",
    "    \n",
    "    def _convert(self, type_:str):\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "        if type_ == \"user\":\n",
    "            data = self.user_data\n",
    "        elif type_ == \"item\":\n",
    "            data = self.item_data\n",
    "        else:\n",
    "            data = self.inter_data\n",
    "        data.columns = self._feat_type_wrap(type_)\n",
    "        data.to_csv(os.path.join(self.output_dir, f\"{self.dataset_name}.{type_}\"), index=False)\n",
    "        \n",
    "    def fit(self):\n",
    "        for type_ in [\"user\", \"item\", \"inter\"]:\n",
    "            self._convert(type_)\n",
    "        \n",
    "    \n",
    "wc = WSDreamDataConvert(wsdream_type=WSDreamDataType.RT_ONLY)\n",
    "# wc.load_user_data()\n",
    "wc.fit()\n",
    "\n",
    "# data = pd.read_csv(os.path.join(DATASET_DIR, \"rt.inter\"), sep=\",\", header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "from torch.amp.autocast_mode import autocast\n",
    "from torch.cuda.amp.grad_scaler import GradScaler\n",
    "import time\n",
    "from recbole.utils import early_stopping, dict2str, ensure_dir, get_gpu_usage, get_local_time, set_color\n",
    "\n",
    "class AbstractTrainer(object):\n",
    "    r\"\"\"Trainer Class is used to manage the training and evaluation processes of recommender system models.\n",
    "    AbstractTrainer is an abstract class in which the fit() and evaluate() method should be implemented according\n",
    "    to different training and evaluation strategies.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, model):\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, train_data):\n",
    "        r\"\"\"Train the model based on the train data.\"\"\"\n",
    "        raise NotImplementedError(\"Method [next] should be implemented.\")\n",
    "\n",
    "    def evaluate(self, eval_data):\n",
    "        r\"\"\"Evaluate the model based on the eval data.\"\"\"\n",
    "\n",
    "        raise NotImplementedError(\"Method [next] should be implemented.\")\n",
    "\n",
    "\n",
    "\n",
    "class Trainer(AbstractTrainer):\n",
    "    r\"\"\"The basic Trainer for basic training and evaluation strategies in recommender systems. This class defines common\n",
    "    functions for training and evaluation processes of most recommender system models, including fit(), evaluate(),\n",
    "    resume_checkpoint() and some other features helpful for model training and evaluation.\n",
    "\n",
    "    Generally speaking, this class can serve most recommender system models, If the training process of the model is to\n",
    "    simply optimize a single loss without involving any complex training strategies, such as adversarial learning,\n",
    "    pre-training and so on.\n",
    "\n",
    "    Initializing the Trainer needs two parameters: `config` and `model`. `config` records the parameters information\n",
    "    for controlling training and evaluation, such as `learning_rate`, `epochs`, `eval_step` and so on.\n",
    "    `model` is the instantiated object of a Model Class.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, model):\n",
    "        super(Trainer, self).__init__(config, model)\n",
    "        \n",
    "        self.learner = config[\"learner\"]\n",
    "        self.learning_rate = config[\"learning_rate\"]\n",
    "        self.epochs = config[\"epochs\"]\n",
    "        self.eval_step:int = min(config[\"eval_step\"], self.epochs)\n",
    "        self.stopping_step = config[\"stopping_step\"]\n",
    "        self.valid_metric_bigger = config[\"valid_metric_bigger\"] # 是不是越大越好\n",
    "        self.test_batch_size = config[\"eval_batch_size\"]\n",
    "        self.gpu_available = torch.cuda.is_available() and config[\"use_gpu\"]\n",
    "        self.device = config[\"device\"]\n",
    "        self.checkpoint_dir = config[\"checkpoint_dir\"]\n",
    "        self.enable_amp = config[\"enable_amp\"]\n",
    "        self.enable_scaler = torch.cuda.is_available() and config[\"enable_scaler\"]\n",
    "        self.enable_amp = config[\"enable_amp\"]\n",
    "        ensure_dir(self.checkpoint_dir)\n",
    "        saved_model_file = \"{}-{}.pth\".format(self.config[\"model\"], get_local_time())\n",
    "        self.saved_model_file = os.path.join(self.checkpoint_dir, saved_model_file)\n",
    "        self.weight_decay = config[\"weight_decay\"]\n",
    "\n",
    "        self.start_epoch = 0\n",
    "        self.cur_step = 0\n",
    "        self.best_valid_score = -np.inf if self.valid_metric_bigger else np.inf\n",
    "        self.best_valid_result = None\n",
    "        self.train_loss_dict = dict()\n",
    "        self.optimizer = self._build_optimizer()\n",
    "        self.eval_type = config[\"eval_type\"]\n",
    "        self.item_tensor = None\n",
    "        self.tot_item_num = None\n",
    "\n",
    "    def _build_optimizer(self, **kwargs):\n",
    "        r\"\"\"Init the Optimizer\n",
    "\n",
    "        Args:\n",
    "            params (torch.nn.Parameter, optional): The parameters to be optimized.\n",
    "                Defaults to ``self.model.parameters()``.\n",
    "            learner (str, optional): The name of used optimizer. Defaults to ``self.learner``.\n",
    "            learning_rate (float, optional): Learning rate. Defaults to ``self.learning_rate``.\n",
    "            weight_decay (float, optional): The L2 regularization weight. Defaults to ``self.weight_decay``.\n",
    "\n",
    "        Returns:\n",
    "            torch.optim: the optimizer\n",
    "        \"\"\"\n",
    "        params = kwargs.pop(\"params\", self.model.parameters())\n",
    "        learner = kwargs.pop(\"learner\", self.learner)\n",
    "        learning_rate = kwargs.pop(\"learning_rate\", self.learning_rate)\n",
    "        weight_decay = kwargs.pop(\"weight_decay\", self.weight_decay)\n",
    "\n",
    "        if (\n",
    "            self.config[\"reg_weight\"]\n",
    "            and weight_decay\n",
    "            and weight_decay * self.config[\"reg_weight\"] > 0\n",
    "        ):\n",
    "            ...\n",
    "            # self.logger.warning(\n",
    "            #     \"The parameters [weight_decay] and [reg_weight] are specified simultaneously, \"\n",
    "            #     \"which may lead to double regularization.\"\n",
    "            # )\n",
    "\n",
    "        if learner.lower() == \"adam\":\n",
    "            optimizer = optim.Adam(params, lr=learning_rate, weight_decay=weight_decay)\n",
    "        elif learner.lower() == \"sgd\":\n",
    "            optimizer = optim.SGD(params, lr=learning_rate, weight_decay=weight_decay)\n",
    "        elif learner.lower() == \"adagrad\":\n",
    "            optimizer = optim.Adagrad(\n",
    "                params, lr=learning_rate, weight_decay=weight_decay\n",
    "            )\n",
    "        elif learner.lower() == \"rmsprop\":\n",
    "            optimizer = optim.RMSprop(\n",
    "                params, lr=learning_rate, weight_decay=weight_decay\n",
    "            )\n",
    "        elif learner.lower() == \"sparse_adam\":\n",
    "            optimizer = optim.SparseAdam(params, lr=learning_rate)\n",
    "            if weight_decay > 0:\n",
    "                ...\n",
    "                # self.logger.warning(\n",
    "                #     \"Sparse Adam cannot argument received argument [{weight_decay}]\"\n",
    "                # )\n",
    "        else:\n",
    "            # self.logger.warning(\n",
    "            #     \"Received unrecognized optimizer, set default Adam optimizer\"\n",
    "            # )\n",
    "            optimizer = optim.Adam(params, lr=learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "    def _train_epoch(self, train_data, epoch_idx, loss_func=None, show_progress=False):\n",
    "        r\"\"\"Train the model in an epoch\n",
    "\n",
    "        Args:\n",
    "            train_data (DataLoader): The train data.\n",
    "            epoch_idx (int): The current epoch id.\n",
    "            loss_func (function): The loss function of :attr:`model`. If it is ``None``, the loss function will be\n",
    "                :attr:`self.model.calculate_loss`. Defaults to ``None``.\n",
    "            show_progress (bool): Show the progress of training epoch. Defaults to ``False``.\n",
    "\n",
    "        Returns:\n",
    "            float/tuple: The sum of loss returned by all batches in this epoch. If the loss in each batch contains\n",
    "            multiple parts and the model return these multiple parts loss instead of the sum of loss, it will return a\n",
    "            tuple which includes the sum of loss in each part.\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        loss_func = loss_func or self.model.calculate_loss\n",
    "        total_loss = None\n",
    "        iter_data = (\n",
    "            tqdm(\n",
    "                train_data,\n",
    "                total=len(train_data),\n",
    "                ncols=100,\n",
    "                desc=set_color(f\"Train {epoch_idx:>5}\", \"pink\"),\n",
    "            )\n",
    "            if show_progress\n",
    "            else train_data\n",
    "        )\n",
    "\n",
    "        scaler = GradScaler(enabled=self.enable_scaler)\n",
    "        for batch_idx, interaction in enumerate(iter_data):\n",
    "            interaction = interaction.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            # 自动混合精度\n",
    "            with autocast(device_type=self.device.type, enabled=self.enable_amp):\n",
    "                losses = loss_func(interaction)\n",
    "\n",
    "            if isinstance(losses, tuple):\n",
    "                loss = sum(losses)\n",
    "                loss_tuple = tuple(per_loss.item() for per_loss in losses)\n",
    "                total_loss = (\n",
    "                    loss_tuple\n",
    "                    if total_loss is None\n",
    "                    else tuple(map(sum, zip(total_loss, loss_tuple)))\n",
    "                )\n",
    "            else:\n",
    "                loss = losses\n",
    "                total_loss = (\n",
    "                    losses.item() if total_loss is None else total_loss + losses.item()\n",
    "                )\n",
    "            self._check_nan(loss)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(self.optimizer)\n",
    "            scaler.update()\n",
    "            # if self.gpu_available and show_progress:\n",
    "            #     iter_data.set_postfix_str(\n",
    "            #         set_color(\"GPU RAM: \" + get_gpu_usage(self.device), \"yellow\")\n",
    "            #     )\n",
    "        return total_loss\n",
    "\n",
    "    def _save_checkpoint(self, epoch, verbose=True, **kwargs):\n",
    "        r\"\"\"Store the model parameters information and training information.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): the current epoch id\n",
    "\n",
    "        \"\"\"\n",
    "        if not self.config[\"single_spec\"] and self.config[\"local_rank\"] != 0:\n",
    "            return\n",
    "        saved_model_file = kwargs.pop(\"saved_model_file\", self.saved_model_file)\n",
    "        state = {\n",
    "            \"config\": self.config,\n",
    "            \"epoch\": epoch,\n",
    "            \"cur_step\": self.cur_step,\n",
    "            \"best_valid_score\": self.best_valid_score,\n",
    "            \"state_dict\": self.model.state_dict(),\n",
    "            \"other_parameter\": self.model.other_parameter(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "        }\n",
    "        torch.save(state, saved_model_file, pickle_protocol=4)\n",
    "        if verbose:\n",
    "            ...\n",
    "            # self.logger.info(\n",
    "            #     set_color(\"Saving current\", \"blue\") + f\": {saved_model_file}\"\n",
    "            # )\n",
    "\n",
    "    def resume_checkpoint(self, resume_file):\n",
    "        r\"\"\"Load the model parameters information and training information.\n",
    "\n",
    "        Args:\n",
    "            resume_file (file): the checkpoint file\n",
    "\n",
    "        \"\"\"\n",
    "        resume_file = str(resume_file)\n",
    "        self.saved_model_file = resume_file\n",
    "        checkpoint = torch.load(resume_file, map_location=self.device)\n",
    "        self.start_epoch = checkpoint[\"epoch\"] + 1\n",
    "        self.cur_step = checkpoint[\"cur_step\"]\n",
    "        self.best_valid_score = checkpoint[\"best_valid_score\"]\n",
    "\n",
    "        # load architecture params from checkpoint\n",
    "        if checkpoint[\"config\"][\"model\"].lower() != self.config[\"model\"].lower():\n",
    "            ...\n",
    "            # self.logger.warning(\n",
    "            #     \"Architecture configuration given in config file is different from that of checkpoint. \"\n",
    "            #     \"This may yield an exception while state_dict is being loaded.\"\n",
    "            # )\n",
    "        self.model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        self.model.load_other_parameter(checkpoint.get(\"other_parameter\"))\n",
    "\n",
    "        # load optimizer state from checkpoint only when optimizer type is not changed\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        message_output = \"Checkpoint loaded. Resume training from epoch {}\".format(\n",
    "            self.start_epoch\n",
    "        )\n",
    "        # self.logger.info(message_output)\n",
    "\n",
    "    def _check_nan(self, loss):\n",
    "        if torch.isnan(loss):\n",
    "            raise ValueError(\"Training loss is nan\")\n",
    "\n",
    "    def _generate_train_loss_output(self, epoch_idx, s_time, e_time, losses):\n",
    "        des = self.config[\"loss_decimal_place\"] or 4\n",
    "        train_loss_output = (\n",
    "            set_color(\"epoch %d training\", \"green\")\n",
    "            + \" [\"\n",
    "            + set_color(\"time\", \"blue\")\n",
    "            + \": %.2fs, \"\n",
    "        ) % (epoch_idx, e_time - s_time)\n",
    "        if isinstance(losses, tuple):\n",
    "            des = set_color(\"train_loss%d\", \"blue\") + \": %.\" + str(des) + \"f\"\n",
    "            train_loss_output += \", \".join(\n",
    "                des % (idx + 1, loss) for idx, loss in enumerate(losses)\n",
    "            )\n",
    "        else:\n",
    "            des = \"%.\" + str(des) + \"f\"\n",
    "            train_loss_output += set_color(\"train loss\", \"blue\") + \": \" + des % losses\n",
    "        return train_loss_output + \"]\"\n",
    "\n",
    "    # def _add_train_loss_to_tensorboard(self, epoch_idx, losses, tag=\"Loss/Train\"):\n",
    "    #     if isinstance(losses, tuple):\n",
    "    #         for idx, loss in enumerate(losses):\n",
    "    #             self.tensorboard.add_scalar(tag + str(idx), loss, epoch_idx)\n",
    "    #     else:\n",
    "    #         self.tensorboard.add_scalar(tag, losses, epoch_idx)\n",
    "\n",
    "    # def _add_hparam_to_tensorboard(self, best_valid_result):\n",
    "    #     # base hparam\n",
    "    #     hparam_dict = {\n",
    "    #         \"learner\": self.config[\"learner\"],\n",
    "    #         \"learning_rate\": self.config[\"learning_rate\"],\n",
    "    #         \"train_batch_size\": self.config[\"train_batch_size\"],\n",
    "    #     }\n",
    "    #     # unrecorded parameter\n",
    "    #     unrecorded_parameter = {\n",
    "    #         parameter\n",
    "    #         for parameters in self.config.parameters.values()\n",
    "    #         for parameter in parameters\n",
    "    #     }.union({\"model\", \"dataset\", \"config_files\", \"device\"})\n",
    "    #     # other model-specific hparam\n",
    "    #     hparam_dict.update(\n",
    "    #         {\n",
    "    #             para: val\n",
    "    #             for para, val in self.config.final_config_dict.items()\n",
    "    #             if para not in unrecorded_parameter\n",
    "    #         }\n",
    "    #     )\n",
    "    #     for k in hparam_dict:\n",
    "    #         if hparam_dict[k] is not None and not isinstance(\n",
    "    #             hparam_dict[k], (bool, str, float, int)\n",
    "    #         ):\n",
    "    #             hparam_dict[k] = str(hparam_dict[k])\n",
    "\n",
    "    #     self.tensorboard.add_hparams(\n",
    "    #         hparam_dict, {\"hparam/best_valid_result\": best_valid_result}\n",
    "    #     )\n",
    "\n",
    "    def fit(\n",
    "        self,\n",
    "        train_data,\n",
    "        valid_data=None,\n",
    "        verbose=True,\n",
    "        saved=True,\n",
    "        show_progress=False,\n",
    "        callback_fn=None,\n",
    "    ):\n",
    "        r\"\"\"Train the model based on the train data and the valid data.\n",
    "\n",
    "        Args:\n",
    "            train_data (DataLoader): the train data\n",
    "            valid_data (DataLoader, optional): the valid data, default: None.\n",
    "                                               If it's None, the early_stopping is invalid.\n",
    "            verbose (bool, optional): whether to write training and evaluation information to logger, default: True\n",
    "            saved (bool, optional): whether to save the model parameters, default: True\n",
    "            show_progress (bool): Show the progress of training epoch and evaluate epoch. Defaults to ``False``.\n",
    "            callback_fn (callable): Optional callback function executed at end of epoch.\n",
    "                                    Includes (epoch_idx, valid_score) input arguments.\n",
    "\n",
    "        Returns:\n",
    "             (float, dict): best valid score and best valid result. If valid_data is None, it returns (-1, None)\n",
    "        \"\"\"\n",
    "        if saved and self.start_epoch >= self.epochs:\n",
    "            self._save_checkpoint(-1, verbose=verbose)\n",
    "        \n",
    "        valid_step = 0\n",
    "\n",
    "        for epoch_idx in range(self.start_epoch, self.epochs):\n",
    "            # train\n",
    "            training_start_time = time.time()\n",
    "            train_loss = self._train_epoch(\n",
    "                train_data, epoch_idx, show_progress=show_progress\n",
    "            )\n",
    "            self.train_loss_dict[epoch_idx] = (\n",
    "                sum(train_loss) if isinstance(train_loss, tuple) else train_loss\n",
    "            )\n",
    "            training_end_time = time.time()\n",
    "            train_loss_output = self._generate_train_loss_output(\n",
    "                epoch_idx, training_start_time, training_end_time, train_loss\n",
    "            )\n",
    "            if verbose:\n",
    "                # self.logger.info(train_loss_output)\n",
    "                ...\n",
    "            # TODO\n",
    "            # self._add_train_loss_to_tensorboard(epoch_idx, train_loss)\n",
    "            # self.wandblogger.log_metrics(\n",
    "            #     {\"epoch\": epoch_idx, \"train_loss\": train_loss, \"train_step\": epoch_idx},\n",
    "            #     head=\"train\",\n",
    "            # )\n",
    "\n",
    "            # eval\n",
    "            if self.eval_step <= 0 or not valid_data:\n",
    "                if saved:\n",
    "                    self._save_checkpoint(epoch_idx, verbose=verbose)\n",
    "                continue\n",
    "            # TODO\n",
    "            # if (epoch_idx + 1) % self.eval_step == 0:\n",
    "            #     valid_start_time = time.time()\n",
    "            #     valid_score, valid_result = self._valid_epoch(\n",
    "            #         valid_data, show_progress=show_progress\n",
    "            #     )\n",
    "\n",
    "            #     (\n",
    "            #         self.best_valid_score,\n",
    "            #         self.cur_step,\n",
    "            #         stop_flag,\n",
    "            #         update_flag,\n",
    "            #     ) = early_stopping(\n",
    "            #         valid_score,\n",
    "            #         self.best_valid_score,\n",
    "            #         self.cur_step,\n",
    "            #         max_step=self.stopping_step,\n",
    "            #         bigger=self.valid_metric_bigger,\n",
    "            #     )\n",
    "            #     valid_end_time = time.time()\n",
    "            #     valid_score_output = (\n",
    "            #         set_color(\"epoch %d evaluating\", \"green\")\n",
    "            #         + \" [\"\n",
    "            #         + set_color(\"time\", \"blue\")\n",
    "            #         + \": %.2fs, \"\n",
    "            #         + set_color(\"valid_score\", \"blue\")\n",
    "            #         + \": %f]\"\n",
    "            #     ) % (epoch_idx, valid_end_time - valid_start_time, valid_score)\n",
    "            #     valid_result_output = (\n",
    "            #         set_color(\"valid result\", \"blue\") + \": \\n\" + dict2str(valid_result)\n",
    "            #     )\n",
    "            #     if verbose:\n",
    "            #         # self.logger.info(valid_score_output)\n",
    "            #         # self.logger.info(valid_result_output)\n",
    "            #         ...\n",
    "            #     # self.tensorboard.add_scalar(\"Vaild_score\", valid_score, epoch_idx)\n",
    "            #     # self.wandblogger.log_metrics(\n",
    "            #     #     {**valid_result, \"valid_step\": valid_step}, head=\"valid\"\n",
    "            #     # )\n",
    "\n",
    "            #     if update_flag:\n",
    "            #         if saved:\n",
    "            #             self._save_checkpoint(epoch_idx, verbose=verbose)\n",
    "            #         self.best_valid_result = valid_result\n",
    "\n",
    "            #     if callback_fn:\n",
    "            #         callback_fn(epoch_idx, valid_score)\n",
    "\n",
    "            #     if stop_flag:\n",
    "            #         stop_output = \"Finished training, best eval result in epoch %d\" % (\n",
    "            #             epoch_idx - self.cur_step * self.eval_step\n",
    "            #         )\n",
    "            #         if verbose:\n",
    "            #             # self.logger.info(stop_output)\n",
    "            #             ...\n",
    "            #         break\n",
    "\n",
    "            #     valid_step += 1\n",
    "\n",
    "        # self._add_hparam_to_tensorboard(self.best_valid_score)\n",
    "        return self.best_valid_score, self.best_valid_result\n",
    "\n",
    "\n",
    "    def _spilt_predict(self, interaction, batch_size):\n",
    "        spilt_interaction = dict()\n",
    "        for key, tensor in interaction.interaction.items():\n",
    "            spilt_interaction[key] = tensor.split(self.test_batch_size, dim=0)\n",
    "        num_block = (batch_size + self.test_batch_size - 1) // self.test_batch_size\n",
    "        result_list = []\n",
    "        for i in range(num_block):\n",
    "            current_interaction = dict()\n",
    "            for key, spilt_tensor in spilt_interaction.items():\n",
    "                current_interaction[key] = spilt_tensor[i]\n",
    "            result = self.model.predict(\n",
    "                Interaction(current_interaction).to(self.device)\n",
    "            )\n",
    "            if len(result.shape) == 0:\n",
    "                result = result.unsqueeze(0)\n",
    "            result_list.append(result)\n",
    "        return torch.cat(result_list, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mTrain     0\u001b[0m:   3%|█                                         | 10508/394935 [00:31<19:01, 336.80it/s]\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 28\u001b[0m\n\u001b[1;32m     26\u001b[0m model \u001b[39m=\u001b[39m NeuMF(config, dataset)\n\u001b[1;32m     27\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(config, model)\n\u001b[0;32m---> 28\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(train_data, test_data, saved\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, show_progress\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "Cell \u001b[0;32mIn[3], line 339\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, train_data, valid_data, verbose, saved, show_progress, callback_fn)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[39mfor\u001b[39;00m epoch_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstart_epoch, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs):\n\u001b[1;32m    337\u001b[0m     \u001b[39m# train\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     training_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 339\u001b[0m     train_loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_epoch(\n\u001b[1;32m    340\u001b[0m         train_data, epoch_idx, show_progress\u001b[39m=\u001b[39;49mshow_progress\n\u001b[1;32m    341\u001b[0m     )\n\u001b[1;32m    342\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_loss_dict[epoch_idx] \u001b[39m=\u001b[39m (\n\u001b[1;32m    343\u001b[0m         \u001b[39msum\u001b[39m(train_loss) \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(train_loss, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m train_loss\n\u001b[1;32m    344\u001b[0m     )\n\u001b[1;32m    345\u001b[0m     training_end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[3], line 182\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[0;34m(self, train_data, epoch_idx, loss_func, show_progress)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_nan(loss)\n\u001b[1;32m    181\u001b[0m scaler\u001b[39m.\u001b[39mscale(loss)\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m--> 182\u001b[0m scaler\u001b[39m.\u001b[39;49mstep(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer)\n\u001b[1;32m    183\u001b[0m scaler\u001b[39m.\u001b[39mupdate()\n\u001b[1;32m    184\u001b[0m \u001b[39m# if self.gpu_available and show_progress:\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[39m#     iter_data.set_postfix_str(\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[39m#         set_color(\"GPU RAM: \" + get_gpu_usage(self.device), \"yellow\")\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[39m#     )\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py:315\u001b[0m, in \u001b[0;36mGradScaler.step\u001b[0;34m(self, optimizer, *args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[39m:meth:`step` carries out the following two operations:\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[39m    Closure use is not currently supported.\u001b[39;00m\n\u001b[1;32m    313\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enabled):\n\u001b[0;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    317\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mclosure\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    318\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mClosure use is not currently supported if GradScaler is enabled.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/optim/optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/optim/adam.py:132\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    129\u001b[0m     state_steps \u001b[39m=\u001b[39m []\n\u001b[1;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m--> 132\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init_group(\n\u001b[1;32m    133\u001b[0m         group,\n\u001b[1;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m    135\u001b[0m         grads,\n\u001b[1;32m    136\u001b[0m         exp_avgs,\n\u001b[1;32m    137\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    139\u001b[0m         state_steps)\n\u001b[1;32m    141\u001b[0m     adam(\n\u001b[1;32m    142\u001b[0m         params_with_grad,\n\u001b[1;32m    143\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39m\u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mfound_inf\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/gnn/lib/python3.9/site-packages/torch/optim/adam.py:81\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39mif\u001b[39;00m p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mis_sparse:\n\u001b[1;32m     80\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAdam does not support sparse gradients, please consider SparseAdam instead\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 81\u001b[0m grads\u001b[39m.\u001b[39;49mappend(p\u001b[39m.\u001b[39mgrad)\n\u001b[1;32m     83\u001b[0m state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate[p]\n\u001b[1;32m     84\u001b[0m \u001b[39m# Lazy state initialization\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from data.dataloader import GeneralDataLoader\n",
    "from models.neumf import NeuMF\n",
    "\n",
    "def data_reparation(config, dataset:RecboleDataset):\n",
    "    built_dataset = dataset.build()\n",
    "    train_dataset, test_dataset = built_dataset\n",
    "    train_data = GeneralDataLoader(train_dataset, config)\n",
    "    test_data = GeneralDataLoader(test_dataset, config)\n",
    "    return train_data, test_data\n",
    "\n",
    "from config.configuration import Config\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\n",
    "        \"--dataset\", \"-d\", type=str, default=\"wsdream-rt\", help=\"name of datasets\"\n",
    "    )\n",
    "\n",
    "args, _ = parser.parse_known_args()\n",
    "\n",
    "config = Config(model=\"NeuMF\", dataset=args.dataset)\n",
    "    \n",
    "\n",
    "dataset = GeneralDataset(config)\n",
    "train_data, test_data = dataset.build()\n",
    "model = NeuMF(config, dataset)\n",
    "trainer = Trainer(config, model)\n",
    "trainer.fit(train_data, test_data, saved=False, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
